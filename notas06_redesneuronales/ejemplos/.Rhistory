w1 = seq(0,0.5,length=50)
w2 = w1
#Punto 1 (3; 2; 1)
#Punto 2 (1; 4; 1)
#Punto 3 (-1; 1; 0)
#Punto 4 (-1; -1; 0)
#Punto 5 (2; -1; 0)
# Definimos la funci?n que dibujaremos
f1 = function(w1,w2)
{(1 - (2*w1 + w2*3))^2+ (1 -(w1 + w2*4))^2 + (0 - ((-1)*w1 + w2))^2 +
(0 - ((-1)*w1 + (-1)*w2))^2 + (0 - (2*w1 + (-1)*w2))^2}
z = outer(w1,w2,f1)
#z = f(x, y)
# Un gr?fico en perspectiva
persp(w1,w2,z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
f2 = function(w1,w2)
{(1 - (2*w1 + w2*3))^2+ (1 -(w1 + w2*4))^2 + (0 - ((-1)*w1 + w2))^2 +
(1 - ((-1)*w1 + (-1)*w2))^2 + (0 - (2*w1 + (-1)*w2))^2}
# La función outer evalua la funci?n f en cada
z = outer(w1,w2,f2)
#z = f(x, y)
# Un gráfico en perspectiva
persp(w1,w2,z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
fopt1 <- function(x)
{
w1 <- x[1]
w2 <- x[2]
(1 - (2*w1 + w2*3))^2+ (1 -(w1 + w2*4))^2 + (0 - ((-1)*w1 + w2))^2 +
(0 - ((-1)*w1 + (-1)*w2))^2 + (0 - (2*w1 + (-1)*w2))^2
}
optim(c(0,0), fopt1)
fopt1 <- function(x)
{
w1 <- x[1]
w2 <- x[2]
w0 <- x[3]
(1 - (2*w1 + w2*3))^2+
(1 -(w1 + w2*4))^2 +
(0 - ((-1)*w1 + w2))^2 +
(0 - ((-1)*w1 + (-1)*w2))^2 +
(0 - (2*w1 + (-1)*w2))^2
}
optim(c(0,0,0), fopt1)
fopt1 <- function(x)
{
w1 <- x[1]
w2 <- x[2]
w0 <- x[3]
(1 - (2*w1 + w2*3 + w0))^2+
(1 -(w1 + w2*4 + w0))^2 +
(0 - ((-1)*w1 + w2 + w0))^2 +
(0 - ((-1)*w1 + (-1)*w2 + w0))^2 +
(0 - (2*w1 + (-1)*w2 + w0))^2
}
optim(c(0,0,0), fopt1)
fopt1 <- function(x)
{
w1 <- x[1]
w2 <- x[2]
w0 <- x[3]
(1 - (2*w1 + w2*3 + w0))^2+
(1 -(w1 + w2*4 + w0))^2 +
(0 - ((-1)*w1 + w2 + w0))^2 +
(0 - ((-1)*w1 + (-1)*w2 + w0))^2 +
(0 - (2*w1 + (-1)*w2 + w0))^2
}
optim(c(0,0,0), fopt1)
w1 = seq(-10,10,length=50)
w2 = w1
f1 = function(w1,w2)
{
(1 - 1/(1+exp(2*w1 + w2*3)))^2+
(1 - 1/(1+exp(w1 + w2*4)))^2 +
(0 - 1/(1+exp((-1)*w1 + w2)))^2 +
(0 - 1/(1+exp((-1)*w1 + (-1)*w2)))^2 +
(0 - 1/(1+exp(2*w1 + (-1)*w2)))^2
}
z = outer(w1,w2,f1)
persp(w1,w2,z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
w1 = seq(0,0.5,length=50)
w2 = w1
#Punto 1 (3; 2; 1)
#Punto 2 (1; 4; 1)
#Punto 3 (-1; 1; 0)
#Punto 4 (-1; -1; 0)
#Punto 5 (2; -1; 0)
#Definimos la función (en este caso cuadrática) que dibujaremos
f1 = function(w1,w2)
{
(1 - (2*w1 + w2*3))^2+
(1 -(w1 + w2*4))^2 +
(0 - ((-1)*w1 + w2))^2 +
(0 - ((-1)*w1 + (-1)*w2))^2 +
(0 - (2*w1 + (-1)*w2))^2
}
z = outer(w1,w2,f1)
persp(w1,w2,z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
library(neuralnet)
X = data.frame(x1, x2)
x1 = (1, 1, 0, 0)
x1
x1 = c(1, 1, 0, 0)
x2 = c(1, 0, 1, 0)
y = c(0, 1, 1, 0)
X = data.frame(x1, x2)
X
help("neuralnet")
nn <- neuralnet(y ~ x1 + x2, hidden = 2)
X = data.frame(x1, x2, y)
nn <- neuralnet(y ~ x1 + x2, data = X,hidden = 2)
plot(nn)
predict(nn, c(1, 1))
Pred = X[1,]
predict(nn, Pred)
X[1,]
Pred = X[1,]
predict(nn, Pred)
Pred = X[1,]
X[1,]
predict(nn, Pred)
Pred = X[2,]
X[2,]
predict(nn, Pred)
Pred = X[3,]
X[3,]
predict(nn, Pred)
Pred = X[4,]
X[4,]
predict(nn, Pred)
nn <- neuralnet(y ~ x1 + x2, data = X)
plot(nn)
Pred = X[4,]
X[4,]
predict(nn, Pred)
Pred = X[3,]
X[3,]
predict(nn, Pred)
Pred = X[2,]
X[2,]
predict(nn, Pred)
Pred = X[1,]
X[1,]
predict(nn, Pred)
nn <- neuralnet(y ~ x1 + x2, hidden = 0, data = X)
plot(nn)
Pred = X[1,]
X[1,]
predict(nn, Pred)
Pred = X[2,]
X[2,]
predict(nn, Pred)
Pred = X[3,]
X[3,]
predict(nn, Pred)
Pred = X[4,]
X[4,]
predict(nn, Pred)
X[3,]
Pred = X[3,]
X[3,]
predict(nn, Pred)
library(neuralnet)
#Trabla de verdad de la función XOR
x1 = c(1, 1, 0, 0)
x2 = c(1, 0, 1, 0)
y = c(0, 1, 1, 0)
X = data.frame(x1, x2, y)
X
x1 = c(1, 1, 0, 0)
x2 = c(1, 0, 1, 0)
y = c(0, 1, 1, 0)
X = data.frame(x1, x2, y)
X
#Modelo neuronal de una única capa (entradas - salida)
nn1 <- neuralnet(y ~ x1 + x2,
hidden = 0,
data = X)
plot(nn1)
#Modelo neuronal de una única capa (entradas - salida)
set.seed(1234)
nn1 <- neuralnet(y ~ x1 + x2,
hidden = 0,
data = X)
plot(nn1)
set.seed(1234)
nn1 <- neuralnet(y ~ x1 + x2,
hidden = 0,
data = X)
plot(nn1)
Pred = X[1,]
X[1,]
predict(nn1, Pred)
Pred = X[2,]
X[2,]
predict(nn1, Pred)
Pred = X[3,]
X[3,]
predict(nn1, Pred)
Pred = X[4,]
X[4,]
predict(nn1, Pred)
nn2 <- neuralnet(y ~ x1 + x2,
hidden = 0,
data = X)
plot(nn2)
set.seed(1234)
nn2 <- neuralnet(y ~ x1 + x2,
hidden = 0,
data = X)
plot(nn2)
set.seed(1234)
nn2 <- neuralnet(y ~ x1 + x2,
hidden = 2,
data = X)
plot(nn2)
#Trabla de verdad de la función XOR
x1 = c(1, 1, 0, 0)
x2 = c(1, 0, 1, 0)
y = c(1, 0, 0, 0)
X2 = data.frame(x1, x2, y)
set.seed(1234)
nn3 <- neuralnet(y ~ x1 + x2,
hidden = 0,
data = X2)
plot(nn3)
Pred = X2[1,]
X2[1,]
predict(nn3, Pred)
Pred = X2[2,]
X2[2,]
predict(nn3, Pred)
Pred = X2[3,]
X2[3,]
predict(nn3, Pred)
Pred = X2[4,]
X2[4,]
predict(nn3, Pred)
library(png)
library(grDevices)
source("hopfield_tools/hopfield.R")
weights = matrix(c(0, -1, 1, 1,-1, 0, -1, -1, 1, -1, 0, 1, 1, -1, 1, 0), 4, 4)
hopnet = list(weights = weights)
weights
init.y = c(-1, -1, -1, -1)
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=F, topo=c(4,1)))
}
init.y = c(-1, 1, -1, -1)
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=F, topo=c(4,1)))
}
init.y = c(-1, -1, 1, -1) #Inestable
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=F, topo=c(4,1)))
}
init.y = c(-1, -1, 1, 1) #Estable a 1 -1 1 1
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=F, topo=c(4,1)))
}
init.y = c(1, -1, 1, 1)
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=F, topo=c(4,1)))
}
init.y = c(1, 1, 1, 1)
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=F, topo=c(4,1)))
}
hopnet
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 100, stepbystep=T, topo=c(4,1)))
}
for (i in 1:20){
print(run.hopfield(hopnet, init.y, maxit = 10, stepbystep=T, topo=c(4,1)))
}
print(run.hopfield(hopnet, init.y, maxit = 10, stepbystep=T, topo=c(4,1)))
print(run.hopfield(hopnet, init.y, maxit = 20, stepbystep=T, topo=c(4,1)))
print(run.hopfield(hopnet, init.y, maxit = 50, stepbystep=T, topo=c(4,1)))
replace
converge
init.y = c(-1, -1, -1, -1)
print(run.hopfield(hopnet, init.y, maxit = 50, stepbystep=T, topo=c(4,1)))
init.y = c(-1, -1, 1, -1) #Inestable
print(run.hopfield(hopnet, init.y, maxit = 50, stepbystep=T, topo=c(4,1)))
init.y = c(-1, -1, 1, -1) #Inestable
print(run.hopfield(hopnet, init.y, maxit = 50, stepbystep=T, topo=c(4,1)))
init.y = c(-1, -1, 1, -1) #Inestable
print(run.hopfield(hopnet, init.y, maxit = 50, stepbystep=T, topo=c(4,1)))
library(png)
digits = c(1,2,3,4,5,6,7,8,9,0)
patterns <- list()
orgdim <- NULL
for (i in digits) {
img <- readPNG(paste(as.character(i),'.png',sep=''))
orgdim <- dim(img)
dim(img) <- NULL
img <- img*2 - 1
patterns[[length(patterns)+1]] <- c(img,1)	# the last 1 for bias
}
similar = matrix(rep(0,length(digits) * length(digits)), length(digits), length(digits))
for (i in 1:length(patterns)) {
for (j in 1:length(patterns)) {
similar[i,j] = patterns[[i]] %*% patterns[[j]] / (sqrt(sum(patterns[[i]] ^ 2)) * sqrt(sum(patterns[[j]] ^ 2)))
}
}
library(gbp)
it <- data.table::data.table(
oid = c(1428571L, 1428571L, 1428571L, 1428572L, 1428572L, 1428572L, 1428572L, 1428572L),
sku = c("A0A0A0", "A0A0A1", "A0A0A1", "A0A0A0", "A0A0A1", "A0A0A1", "A0A0A2", "A0A0A3"),
l   = c(2.140000, 7.240000, 7.240000, 2.140000, 7.240000, 7.240000, 6.000000, 4.000000),
d   = c(3.580000, 7.240000, 7.240000, 3.580000, 7.240000, 7.240000, 6.000000, 4.000000),
h   = c(4.760000, 2.580000, 2.580000, 4.760000, 2.580000, 2.580000, 6.000000, 4.000000),
w   = c(243.0000, 110.0000, 110.0000, 243.0000, 110.0000, 110.0000, 235.0000, 258.0000)
)
it
knitr::kable(it)
bn <- data.table::data.table(
id = c("K0001", "K0002", "K0003", "K0004", "K0005"),
l  = c(06.0000, 10.0000, 09.0000, 10.0000, 22.0000),
d  = c(06.0000, 08.0000, 08.0000, 10.0000, 14.0000),
h  = c(06.0000, 06.0000, 07.0000, 10.0000, 09.0000),
w  = c(600.000, 600.000, 800.000, 800.000, 800.000)
)
knitr::kable(bn)
sn <- gbp::bpp_solver(it = it, bn = bn)
ldhw <- t(as.matrix(it[oid == 1428572L, .(l, d, h, w)]))
ldhw
m <- t(as.matrix(bn[ , .(l, d, h, w)])) # multple bin
m
p <- gbp4d_solver_dpp_prep_create_p(ldhw, m[, 4L]) # single bin
p
sn4d <- gbp4d_solver_dpp(p, ldhw, m[, 4L])
sn4d$it
sn4d$k  # indicator of which items are fitted into bin
gbp4d_viewer(sn4d)
ldhw
m
m
p <- gbp4d_solver_dpp_prep_create_p(ldhw, m[, 4L]) # single bin
p
m[, 4L]
ldhw
sn4d <- gbp4d_solver_dpp(p, ldhw, m[, 4L])
sn4d
help(gbd4d_solver_dpp_prep_create_p)
help(gbp4d_solver_dpp_prep_create_p)
sn4d$it
sn4d
sn4d$it
sn4d$k
sn4d$k  # indicator of which items are fitted into bin
gbp4d_viewer(sn4d)
m[, 4L]
library(markovchain)
library(diagram)
set.seed(1)
States <- c("Rainy","Cloudy","Sunny")
TransMat <- matrix(c(0.30,0.50,0.20,0.25,0.4,0.35,0.1,0.2,0.70),
nrow = 3,
byrow= TRUE,
dimnames = list(States,States))
TransMat
MarkovChainModel <- new("markovchain",
transitionMatrix=TransMat,
states=States,
byrow = TRUE,
name="MarkovChainModel")
MarkovChainModel
plot(MarkovChainModel,package="diagram")
StartState<-c(0,0,1)
Pred3Days <- StartState * (MarkovChainModel ^ 3)
print (round(Pred3Days, 3))
Pred1Week <- StartState * (MarkovChainModel ^ 7)
print (round(Pred1Week, 3))
#Distribución estacionaria: convergencia general del proceso de markov planteado
steadyStates(MarkovChainModel)
library(contextual)
install.packages("contextual")
library(devtools)
install_github('Nth-iteration-labs/contextual')
install_github('Nth-iteration-labs/contextual')
# vector of impressions per variant
b_Sent<-c(1000, 1000, 100)
# vector of responses per variant
b_Reward<-c(100, 110, 10)
msgs<-length(b_Sent)
# number of simulations
N<-5000
# simulation of Beta distributions (success+1, failures+1)
B<-matrix(rbeta(N*msgs, b_Reward+1, (b_Sent-b_Reward)+1),N, byrow = TRUE)
# Take the percentage where each variant
# was observed with the highest rate rate
P<-table(factor(max.col(B), levels=1:ncol(B)))/dim(B)[1]
P
B
b_Reward
P
help(rbeta)
set.seed(155)
B <- matrix(rbeta(N*msgs, b_Reward+1, (b_Sent-b_Reward)+1),N, byrow = TRUE)
# was observed with the highest rate rate
P <- table(factor(max.col(B), levels=1:ncol(B)))/dim(B)[1]
P
B
N*msgs
msgs
b_Reward+1
factor(max.col(B)
, levels=1:ncol(B))
table(factor(max.col(B), levels=1:ncol(B)))
B
table(factor(max.col(B), levels=1:ncol(B)))
b_Reward
b_Sent-b_Reward
N*msgs
P
b_Sent<-c(1000, 1000, 100)
b_Sent<-c(1000, 800, 700)
b_Reward <- c(100, 110, 10)
b_Reward <- c(110, 120, 130)
msgs <- length(b_Sent)
# number of simulations
N <- 5000
# simulation of Beta distributions (success+1, failures+1)
set.seed(155)
B <- matrix(rbeta(N*msgs, b_Reward+1, (b_Sent-b_Reward)+1),
N,
byrow = TRUE)
# Take the percentage where each variant
# was observed with the highest rate rate
P <- table(factor(max.col(B), levels=1:ncol(B)))/dim(B)[1]
P
msgs <- length(b_Sent)
# number of simulations
N <- 5000
# simulation of Beta distributions (success+1, failures+1)
set.seed(155)
B <- matrix(rbeta(N*msgs, b_Reward+1, (b_Sent-b_Reward)+1),
N,
byrow = TRUE)
# Take the percentage where each variant
# was observed with the highest rate rate
P <- table(factor(max.col(B), levels=1:ncol(B)))/dim(B)[1]
P
